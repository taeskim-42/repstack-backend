# frozen_string_literal: true

require_relative "constants"
require_relative "llm_gateway"

module AiTrainer
  # Handles general fitness-related chat using LLM Gateway
  # Routes to cost-efficient models for conversational queries
  # Enhanced with RAG (Retrieval Augmented Generation) from YouTube fitness knowledge
  # Uses Prompt Caching for cost-efficient conversation history
  class ChatService
    include Constants

    HISTORY_LIMIT = 30  # Max messages to include in context
    CACHE_LIMIT = 3     # Messages to cache (Anthropic allows max 4 total, 1 for system prompt)

    class << self
      def general_chat(user:, message:, session_id: nil)
        new(user: user, session_id: session_id).general_chat(message)
      end
    end

    def initialize(user:, session_id: nil)
      @user = user
      @session_id = session_id || generate_session_id
    end

    def general_chat(message)
      # Save user message
      save_message(role: "user", content: message)

      # Retrieve relevant knowledge from YouTube fitness channels
      knowledge_context = retrieve_knowledge(message)

      # Build messages with conversation history (with caching)
      messages = build_messages_with_history(message)

      # Build system prompt
      system_prompt = build_system_prompt(knowledge_context)

      # Call LLM with conversation history and caching
      response = LlmGateway.chat(
        prompt: message,
        task: :general_chat,
        messages: messages,
        system: system_prompt,
        cache_system: true
      )

      if response[:success]
        assistant_message = response[:content].strip

        # Save assistant response
        save_message(role: "assistant", content: assistant_message)

        {
          success: true,
          message: assistant_message,
          model: response[:model],
          knowledge_used: knowledge_context[:used],
          session_id: @session_id,
          cache_stats: {
            cache_read_tokens: response.dig(:usage, :cache_read_input_tokens),
            cache_creation_tokens: response.dig(:usage, :cache_creation_input_tokens)
          }
        }
      else
        { success: false, message: "ì£„ì†¡í•´ìš”, ì ì‹œ ë¬¸ì œê°€ ìƒê²¼ì–´ìš”. ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!" }
      end
    rescue StandardError => e
      Rails.logger.error("ChatService error: #{e.message}\n#{e.backtrace.first(3).join("\n")}")
      { success: false, message: "ì£„ì†¡í•´ìš”, ì ì‹œ ë¬¸ì œê°€ ìƒê²¼ì–´ìš”. ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!" }
    end

    private

    attr_reader :user, :session_id

    def save_message(role:, content:)
      ChatMessage.create!(
        user: user,
        role: role,
        content: content,
        session_id: @session_id
      )
    rescue StandardError => e
      Rails.logger.warn("Failed to save chat message: #{e.message}")
    end

    def build_messages_with_history(new_message)
      # Get recent conversation history
      history = ChatMessage.recent_for_user(
        user.id,
        limit: HISTORY_LIMIT,
        session_id: @session_id
      )

      messages = []

      # Add history with caching on older messages
      history.each_with_index do |msg, idx|
        # Cache older messages (not the last 2 which change frequently)
        should_cache = idx < [history.length - 2, CACHE_LIMIT].min
        messages << msg.to_api_format(cache: should_cache)
      end

      messages
    end

    def generate_session_id
      # Session lasts for 30 minutes of inactivity
      last_message = ChatMessage.where(user_id: user.id).order(created_at: :desc).first

      if last_message && last_message.created_at > 30.minutes.ago
        last_message.session_id
      else
        "session_#{user.id}_#{Time.current.to_i}"
      end
    end

    def retrieve_knowledge(message)
      # Extract keywords from message and search RAG
      keywords = extract_keywords(message)
      knowledge_chunks = search_with_keywords(keywords)

      if knowledge_chunks.any?
        {
          used: true,
          prompt: RagSearchService.build_context_prompt(knowledge_chunks),
          sources: knowledge_chunks.map { |c| c[:source] }.compact
        }
      else
        { used: false, prompt: "", sources: [] }
      end
    rescue StandardError => e
      Rails.logger.warn("RAG search failed: #{e.message}")
      { used: false, prompt: "", sources: [] }
    end

    def extract_keywords(message)
      # Remove common Korean particles and extract meaningful words
      stopwords = %w[ì€ ëŠ” ì´ ê°€ ì„ ë¥¼ ì˜ ì— ì—ì„œ ìœ¼ë¡œ ë¡œ ì™€ ê³¼ í•˜ê³  ì´ê³  ë¼ê³  ë­ë¼ê³  ë­ ë¬´ì—‡ ì–´ë–»ê²Œ ì–´ë–¤ ì™œ ì–¸ì œ ì¢€ ì˜ ë”]
      words = message.gsub(/[?!.,]/, "").split(/\s+/)

      keywords = []

      words.each do |word|
        next if word.length < 2

        # Add original word
        keywords << word

        # Try removing common suffixes
        stopwords.each do |sw|
          if word.end_with?(sw) && word.length > sw.length + 1
            keywords << word.chomp(sw)
          end
        end
      end

      keywords.uniq.reject { |w| w.length < 2 }
    end

    def search_with_keywords(keywords)
      return [] if keywords.empty?

      all_results = []

      # Search each keyword
      keywords.first(5).each do |keyword|
        results = RagSearchService.search(keyword, limit: 2)
        all_results.concat(results)
      end

      # Deduplicate and limit
      all_results.uniq { |r| r[:id] }.first(5)
    end

    # Build system prompt for conversation (cached for efficiency)
    def build_system_prompt(knowledge_context)
      user_level = user.user_profile&.numeric_level || 1
      user_tier = Constants.tier_for_level(user_level)

      prompt_parts = []

      prompt_parts << <<~INTRO
        ë‹¹ì‹ ì€ ì¹œê·¼í•œ AI í”¼íŠ¸ë‹ˆìŠ¤ íŠ¸ë ˆì´ë„ˆì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§§ê³  ë„ì›€ë˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

        ## ì‚¬ìš©ì ì •ë³´
        - ë ˆë²¨: #{user_level}/8 (#{user_tier})
        - ì´ë¦„: #{user.name || 'íšŒì›'}
      INTRO

      # Add RAG knowledge if available
      if knowledge_context[:used] && knowledge_context[:prompt].present?
        prompt_parts << knowledge_context[:prompt]
      end

      prompt_parts << <<~RULES
        ## ê·œì¹™
        1. ë‹¹ì‹ ì€ ìš´ë™ ì „ë¬¸ AI íŠ¸ë ˆì´ë„ˆì…ë‹ˆë‹¤
        2. ìš´ë™ ê´€ë ¨ ì§ˆë¬¸ì—ëŠ” ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
        3. ìš´ë™ ì™¸ ì§ˆë¬¸ì—ëŠ” ì§§ê²Œ ë‹µí•˜ê³ , ìì—°ìŠ¤ëŸ½ê²Œ ìš´ë™/ê±´ê°• ì£¼ì œë¡œ ëŒ€í™”ë¥¼ ìœ ë„í•˜ì„¸ìš”
           ì˜ˆì‹œ: "í”¼ì ë¨¹ê³  ì‹¶ì–´" â†’ "í”¼ì ë§›ìˆì£ ! ğŸ• ìš´ë™ í›„ì— ë“œì‹œë©´ ì£„ì±…ê° ì—†ì´ ì¦ê¸¸ ìˆ˜ ìˆì–´ìš”. ì˜¤ëŠ˜ ë£¨í‹´ì€ í™•ì¸í•˜ì…¨ë‚˜ìš”?"
           ì˜ˆì‹œ: "ì£¼ì‹ ì¶”ì²œí•´ì¤˜" â†’ "ì €ëŠ” ìš´ë™ ì „ë¬¸ì´ë¼ ì£¼ì‹ì€ ì˜ ëª¨ë¥´ê² ì–´ìš” ğŸ˜… ëŒ€ì‹  ì˜¤ëŠ˜ ìš´ë™ ê³„íš ì„¸ì›Œë“œë¦´ê¹Œìš”?"
        4. ì¹œê·¼í•˜ê³  ê²©ë ¤í•˜ëŠ” í†¤ì„ ìœ ì§€í•˜ì„¸ìš”
        5. ë‹µë³€ì€ 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ
        6. ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì„¸ìš”
        7. ì‚¬ìš©ì ë ˆë²¨ì— ë§ëŠ” ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”
        8. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë§¥ë½ì— ë§ê²Œ ë‹µë³€í•˜ì„¸ìš”

        ## âš ï¸ ë§¤ìš° ì¤‘ìš”: ë§¥ë½ ì´í•´
        - ë‹¹ì‹ ì´ ì§ˆë¬¸ì„ í–ˆë‹¤ë©´, ì‚¬ìš©ìì˜ ë‹¤ìŒ ë‹µë³€ì€ **ê·¸ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€**ì…ë‹ˆë‹¤
        - ì˜ˆì‹œ:
          - ë‹¹ì‹ : "ì˜¤ëŠ˜ ì»¨ë””ì…˜ì€ ì–´ë– ì„¸ìš”?" â†’ ì‚¬ìš©ì: "ì•„ì£¼ ì¢‹ì•„" â†’ ì´ê²ƒì€ **ì»¨ë””ì…˜ì´ ì¢‹ë‹¤ëŠ” ë‹µë³€**ì…ë‹ˆë‹¤
          - ë‹¹ì‹ : "ì˜¤ëŠ˜ ìš´ë™ ê³„íš ìˆìœ¼ì„¸ìš”?" â†’ ì‚¬ìš©ì: "ë„¤" â†’ ì´ê²ƒì€ **ìš´ë™ ê³„íšì´ ìˆë‹¤ëŠ” ë‹µë³€**ì…ë‹ˆë‹¤
        - ì‚¬ìš©ìê°€ ì§§ê²Œ ë‹µë³€í•´ë„ (ì˜ˆ: "ì¢‹ì•„", "ë„¤", "ì•„ë‹ˆìš”", "í”¼ê³¤í•´") ì§ì „ ëŒ€í™” ë§¥ë½ì—ì„œ ì˜ë¯¸ë¥¼ íŒŒì•…í•˜ì„¸ìš”
        - ë§¥ë½ ì—†ì´ ë‹¨ì–´ë§Œ ë³´ê³  ì—‰ëš±í•œ í•´ì„ì„ í•˜ì§€ ë§ˆì„¸ìš”

        ìœ„ ê·œì¹™ì— ë”°ë¼ ì¹œê·¼í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”. JSON í˜•ì‹ ì—†ì´ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ì²´ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
      RULES

      prompt_parts.join("\n")
    end
  end
end
